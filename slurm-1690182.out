## SLURM PROLOG ###############################################################
##    Job ID : 1690182
##  Job Name : run_taglets-ps.sh
##  Nodelist : gpu1404
##      CPUs : 1
##  Mem/Node : 65536 MB
## Directory : /gpfs/data/sbach/bats/projects/taglets-project/taglets-pseudoshots
##   Started : Tue Jul  6 10:59:10 EDT 2021
###############################################################################
module: loading 'cuda/9.2.148'
module: unloading 'python/2.7.12'
module: loading 'python/3.7.4'
/users/rbriden/data/rbriden/anaconda3/lib/python3.8/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./run_jpl.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29500
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_4hhgen9c/none_yuwr6u5k
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/users/rbriden/data/rbriden/anaconda3/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_4hhgen9c/none_yuwr6u5k/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_4hhgen9c/none_yuwr6u5k/attempt_0/1/error.json
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[06/Jul/2021 10:59:20] INFO - Problem type: image_classification
[06/Jul/2021 10:59:20] INFO - Problem type: image_classification
Traceback (most recent call last):
  File "./run_jpl.py", line 4, in <module>
[06/Jul/2021 10:59:20] INFO - Dataset dir: /lwll/development
    jpl.main()
  File "/gpfs/data/sbach/bats/projects/taglets-project/taglets-pseudoshots/taglets/task/jpl.py", line 859, in main
Traceback (most recent call last):
  File "./run_jpl.py", line 4, in <module>
    jpl.main()
  File "/gpfs/data/sbach/bats/projects/taglets-project/taglets-pseudoshots/taglets/task/jpl.py", line 881, in main
    log.info(f"Problem type: {problem_type}")
  File "/users/rbriden/data/rbriden/anaconda3/lib/python3.8/logging/__init__.py", line 1434, in info
    raise Exception('`dataset_dir` does not exist..')
Exception: `dataset_dir` does not exist..
    self._log(INFO, msg, args, **kwargs)
  File "/users/rbriden/data/rbriden/anaconda3/lib/python3.8/logging/__init__.py", line 1577, in _log
    self.handle(record)
  File "/users/rbriden/data/rbriden/anaconda3/lib/python3.8/logging/__init__.py", line 1587, in handle
    self.callHandlers(record)
  File "/users/rbriden/data/rbriden/anaconda3/lib/python3.8/logging/__init__.py", line 1649, in callHandlers
    hdlr.handle(record)
  File "/users/rbriden/data/rbriden/anaconda3/lib/python3.8/logging/__init__.py", line 950, in handle
    self.emit(record)
  File "/gpfs/data/sbach/bats/projects/taglets-project/taglets-pseudoshots/taglets/controller.py", line 54, in emit
    self.jpl_logger = logger.log(msg, 'Brown', 0) # For the moment fixed checkpoint
AttributeError: module 'logger' has no attribute 'log'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 22111) of binary: /users/rbriden/data/rbriden/anaconda3/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_4hhgen9c/none_yuwr6u5k/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_4hhgen9c/none_yuwr6u5k/attempt_1/1/error.json
slurmstepd: error: *** JOB 1690182 ON gpu1404 CANCELLED AT 2021-07-06T11:07:53 ***
