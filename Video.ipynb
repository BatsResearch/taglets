{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/a4/41daed85ca7bbb235b6f381d73fd4133512a798d74b38388a77a98308584/pyarrow-4.0.1-cp37-cp37m-manylinux2010_x86_64.whl (20.9MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0MB 18.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /home/ubuntu/snap/jupyter/common/lib/python3.7/site-packages (from pyarrow) (1.20.3)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-4.0.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/snap/jupyter/common/lib/python3.7/site-packages/torchvision/transforms/_functional_video.py:6: UserWarning: The _functional_video module is deprecated. Please use the functional module instead.\n",
      "  \"The _functional_video module is deprecated. Please use the functional module instead.\"\n",
      "/home/ubuntu/snap/jupyter/common/lib/python3.7/site-packages/torchvision/transforms/_transforms_video.py:26: UserWarning: The _transforms_video module is deprecated. Please use the transforms module instead.\n",
      "  \"The _transforms_video module is deprecated. Please use the transforms module instead.\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ")\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "wget.download('https://dl.fbaipublicfiles.com/pytorchvideo/projects/archery.mp4')\n",
    "\n",
    "\n",
    "with open(\"kinetics_classnames.json\", \"r\") as f:\n",
    "    kinetics_classnames = json.load(f)\n",
    "\n",
    "# Create an id to label name mapping\n",
    "kinetics_id_to_classname = {}\n",
    "for k, v in kinetics_classnames.items():\n",
    "    kinetics_id_to_classname[v] = str(k).replace('\"', \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/ubuntu/snap/jupyter/6/.cache/torch/hub/facebookresearch_pytorchvideo_master\n"
     ]
    }
   ],
   "source": [
    "# Device on which to run the model\n",
    "# Set to cuda to load on GPU\n",
    "device = \"cpu\"\n",
    "\n",
    "# Pick a pretrained model and load the pretrained weights\n",
    "model_name = \"slowfast_r50\"\n",
    "model = torch.hub.load(\"facebookresearch/pytorchvideo\", model=model_name, pretrained=True)\n",
    "\n",
    "# Set to eval mode and move to desired device\n",
    "model = model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "feature_extract = True\n",
    "num_classes = 51\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input required by the modelssssss\n",
    "\n",
    "# define function where I can initialize each of the model we might want to use\n",
    "model.blocks[6].proj = nn.Linear(2304, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# SlowFast transform\n",
    "####################\n",
    "\n",
    "# We might want different options for transforming the data depending on the model we have\n",
    "\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api-dev.lollllz.com'\n",
    "secret = 'a5aed2a8-db80-4b22-bf72-11f2d0765572'\n",
    "headers = {'user_secret': secret, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "# This is a convenience for development purposes, IN EVAL ALWAYS USE `full`\n",
    "data_type = 'sample' # can either be `sample` or `full`\n",
    "\n",
    "r = requests.post(f\"{url}/auth/create_session\", json={'session_name': 'testing', 'data_type': data_type, 'task_id': 'problem_test_video_classification'}, headers=headers)\n",
    "r.json()\n",
    "session_token = r.json()['session_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Labels\": [\n",
      "        {\n",
      "            \"class\": \"brush_hair\",\n",
      "            \"end_frame\": 15215,\n",
      "            \"id\": \"154\",\n",
      "            \"start_frame\": 14941\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"cartwheel\",\n",
      "            \"end_frame\": 4049,\n",
      "            \"id\": \"44\",\n",
      "            \"start_frame\": 3970\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"catch\",\n",
      "            \"end_frame\": 1108,\n",
      "            \"id\": \"13\",\n",
      "            \"start_frame\": 1080\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"chew\",\n",
      "            \"end_frame\": 176,\n",
      "            \"id\": \"1\",\n",
      "            \"start_frame\": 54\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"clap\",\n",
      "            \"end_frame\": 3646,\n",
      "            \"id\": \"41\",\n",
      "            \"start_frame\": 3332\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"climb\",\n",
      "            \"end_frame\": 15366,\n",
      "            \"id\": \"156\",\n",
      "            \"start_frame\": 15252\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"climb_stairs\",\n",
      "            \"end_frame\": 3969,\n",
      "            \"id\": \"43\",\n",
      "            \"start_frame\": 3892\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"dive\",\n",
      "            \"end_frame\": 1325,\n",
      "            \"id\": \"16\",\n",
      "            \"start_frame\": 1272\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"draw_sword\",\n",
      "            \"end_frame\": 21169,\n",
      "            \"id\": \"230\",\n",
      "            \"start_frame\": 21075\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"dribble\",\n",
      "            \"end_frame\": 3070,\n",
      "            \"id\": \"36\",\n",
      "            \"start_frame\": 2994\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"drink\",\n",
      "            \"end_frame\": 5177,\n",
      "            \"id\": \"56\",\n",
      "            \"start_frame\": 5044\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"eat\",\n",
      "            \"end_frame\": 5719,\n",
      "            \"id\": \"60\",\n",
      "            \"start_frame\": 5534\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"fall_floor\",\n",
      "            \"end_frame\": 3177,\n",
      "            \"id\": \"38\",\n",
      "            \"start_frame\": 3153\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"fencing\",\n",
      "            \"end_frame\": 881,\n",
      "            \"id\": \"9\",\n",
      "            \"start_frame\": 803\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"flic_flac\",\n",
      "            \"end_frame\": 4903,\n",
      "            \"id\": \"54\",\n",
      "            \"start_frame\": 4826\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"golf\",\n",
      "            \"end_frame\": 18604,\n",
      "            \"id\": \"196\",\n",
      "            \"start_frame\": 18523\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"handstand\",\n",
      "            \"end_frame\": 16215,\n",
      "            \"id\": \"164\",\n",
      "            \"start_frame\": 16137\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"hit\",\n",
      "            \"end_frame\": 19261,\n",
      "            \"id\": \"204\",\n",
      "            \"start_frame\": 19218\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"hug\",\n",
      "            \"end_frame\": 2914,\n",
      "            \"id\": \"34\",\n",
      "            \"start_frame\": 2836\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"jump\",\n",
      "            \"end_frame\": 905,\n",
      "            \"id\": \"10\",\n",
      "            \"start_frame\": 882\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"kick\",\n",
      "            \"end_frame\": 5454,\n",
      "            \"id\": \"58\",\n",
      "            \"start_frame\": 5407\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"kick_ball\",\n",
      "            \"end_frame\": 2709,\n",
      "            \"id\": \"31\",\n",
      "            \"start_frame\": 2664\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"kiss\",\n",
      "            \"end_frame\": 1000,\n",
      "            \"id\": \"11\",\n",
      "            \"start_frame\": 906\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"laugh\",\n",
      "            \"end_frame\": 1933,\n",
      "            \"id\": \"21\",\n",
      "            \"start_frame\": 1810\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"pick\",\n",
      "            \"end_frame\": 1688,\n",
      "            \"id\": \"19\",\n",
      "            \"start_frame\": 1558\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"pour\",\n",
      "            \"end_frame\": 391,\n",
      "            \"id\": \"3\",\n",
      "            \"start_frame\": 221\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"pullup\",\n",
      "            \"end_frame\": 1079,\n",
      "            \"id\": \"12\",\n",
      "            \"start_frame\": 1001\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"punch\",\n",
      "            \"end_frame\": 13678,\n",
      "            \"id\": \"139\",\n",
      "            \"start_frame\": 13568\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"push\",\n",
      "            \"end_frame\": 1809,\n",
      "            \"id\": \"20\",\n",
      "            \"start_frame\": 1689\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"pushup\",\n",
      "            \"end_frame\": 2835,\n",
      "            \"id\": \"33\",\n",
      "            \"start_frame\": 2757\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"ride_bike\",\n",
      "            \"end_frame\": 2527,\n",
      "            \"id\": \"27\",\n",
      "            \"start_frame\": 2450\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"ride_horse\",\n",
      "            \"end_frame\": 2449,\n",
      "            \"id\": \"26\",\n",
      "            \"start_frame\": 2315\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"run\",\n",
      "            \"end_frame\": 1212,\n",
      "            \"id\": \"14\",\n",
      "            \"start_frame\": 1109\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"shake_hands\",\n",
      "            \"end_frame\": 1271,\n",
      "            \"id\": \"15\",\n",
      "            \"start_frame\": 1213\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"shoot_ball\",\n",
      "            \"end_frame\": 16492,\n",
      "            \"id\": \"166\",\n",
      "            \"start_frame\": 16263\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"shoot_bow\",\n",
      "            \"end_frame\": 13948,\n",
      "            \"id\": \"141\",\n",
      "            \"start_frame\": 13758\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"shoot_gun\",\n",
      "            \"end_frame\": 9515,\n",
      "            \"id\": \"105\",\n",
      "            \"start_frame\": 9445\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"sit\",\n",
      "            \"end_frame\": 4618,\n",
      "            \"id\": \"50\",\n",
      "            \"start_frame\": 4542\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"situp\",\n",
      "            \"end_frame\": 562,\n",
      "            \"id\": \"5\",\n",
      "            \"start_frame\": 454\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"smile\",\n",
      "            \"end_frame\": 5899,\n",
      "            \"id\": \"62\",\n",
      "            \"start_frame\": 5833\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"smoke\",\n",
      "            \"end_frame\": 7220,\n",
      "            \"id\": \"81\",\n",
      "            \"start_frame\": 7177\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"somersault\",\n",
      "            \"end_frame\": 4712,\n",
      "            \"id\": \"51\",\n",
      "            \"start_frame\": 4619\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"stand\",\n",
      "            \"end_frame\": 53,\n",
      "            \"id\": \"0\",\n",
      "            \"start_frame\": 0\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"swing_baseball\",\n",
      "            \"end_frame\": 220,\n",
      "            \"id\": \"2\",\n",
      "            \"start_frame\": 177\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"sword\",\n",
      "            \"end_frame\": 2617,\n",
      "            \"id\": \"29\",\n",
      "            \"start_frame\": 2572\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"sword_exercise\",\n",
      "            \"end_frame\": 4108,\n",
      "            \"id\": \"45\",\n",
      "            \"start_frame\": 4050\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"talk\",\n",
      "            \"end_frame\": 2062,\n",
      "            \"id\": \"22\",\n",
      "            \"start_frame\": 1934\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"throw\",\n",
      "            \"end_frame\": 7101,\n",
      "            \"id\": \"79\",\n",
      "            \"start_frame\": 7002\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"turn\",\n",
      "            \"end_frame\": 453,\n",
      "            \"id\": \"4\",\n",
      "            \"start_frame\": 392\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"walk\",\n",
      "            \"end_frame\": 7001,\n",
      "            \"id\": \"78\",\n",
      "            \"start_frame\": 6924\n",
      "        },\n",
      "        {\n",
      "            \"class\": \"wave\",\n",
      "            \"end_frame\": 4760,\n",
      "            \"id\": \"52\",\n",
      "            \"start_frame\": 4713\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "headers = {'user_secret': secret, 'session_token': session_token, 'govteam_secret': os.environ.get('GOVTEAM_SECRET')}\n",
    "\n",
    "r = requests.get(f\"{url}/seed_labels\", headers=headers)\n",
    "print(json.dumps(r.json(), indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "r_data = requests.get(f\"{url}/dataset_metadata/hmdb\", headers=headers)\n",
    "print(len(r_data.json()['dataset_metadata']['classes']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'task_metadata': {'adaptation_dataset': 'hmdb', 'adaptation_evaluation_metrics': ['accuracy'], 'adaptation_label_budget_full': [51, 102, 204, 408, 779, 1486, 2836, 5412], 'adaptation_label_budget_sample': [51, 102, 204, 408, 485, 577, 686, 816], 'base_dataset': 'hmdb', 'base_evaluation_metrics': ['accuracy'], 'base_label_budget_full': [51, 102, 204, 408, 779, 1486, 2836, 5412], 'base_label_budget_sample': [51, 102, 204, 408, 485, 577, 686, 816], 'problem_type': 'video_classification', 'task_id': 'problem_test_video_classification', 'uda_adapt_to_base_overlap_ratio': 1.0, 'uda_base_to_adapt_overlap_ratio': 1.0, 'whitelist': ['imagenet_1k']}}\n"
     ]
    }
   ],
   "source": [
    "r_task = requests.get(f\"{url}/task_metadata/problem_test_video_classification\", headers=headers)\n",
    "print(r_task.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "root = '../../../../../lwll/development/hmdb/'\n",
    "\n",
    "unlabeled_image_path = '../../../../../lwll/development/hmdb/hmdb_sample/train'\n",
    "video = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset used to create dataloaders.\n",
    "    \"\"\"\n",
    "    def __init__(self, filepaths, labels=None, label_map=None, transform=None):\n",
    "        \"\"\"\n",
    "        Create a new CustomImageDataset.\n",
    "        \n",
    "        :param filepaths: A list of filepaths. \n",
    "        :param labels: A list of labels\n",
    "        :param label_map: A dictionary to map string labels to intergers\n",
    "        :param transform: A transform to perform on the images\n",
    "        \"\"\"\n",
    "        self.filepaths = filepaths\n",
    "        self.labels = labels\n",
    "        self.label_map = label_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.filepaths[index]).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            if self.label_map is not None:\n",
    "                label = torch.tensor(self.label_map[(self.labels[index])])\n",
    "            else:\n",
    "                label = torch.tensor(int(self.labels[index]))\n",
    "            return img, label\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "    \n",
    "\n",
    "class CustomVideoDataset(Dataset):\n",
    "    \"\"\"\n",
    "        A custom dataset used to create dataloaders.\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, filepaths, labels=None, label_map=None, transform=None, clips_dictionary=None):\n",
    "        \"\"\"\n",
    "        Create a new CustomVideoDataset.\n",
    "        :param filepaths: A list of filepaths.\n",
    "        :param labels: A list of labels\n",
    "        :param label_map: A dictionary to map string labels to intergers\n",
    "        :param transform: A transform to perform on the frames\n",
    "        :pram clips_dictionary: dictionary (id clip, list images) to get frames of a clip\n",
    "        \"\"\"\n",
    "        self.filepaths = filepaths\n",
    "        self.labels = labels\n",
    "        self.label_map = label_map\n",
    "        self.transform = transform\n",
    "        self.clips_dictionary = clips_dictionary\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        clip_id = str(os.path.basename(self.filepaths[index]))#int(os.path.basename(self.filepaths[index]))  # chech what path you have/want\n",
    "        frames_paths = self.clips_dictionary[clip_id]\n",
    "        # print(f\"FRAMES list[:2]: {frames_paths[:2]} and number of frames {len(frames_paths)}\")\n",
    "        \n",
    "        frames = []\n",
    "        for f in frames_paths:  # get same size clips - random pick for eval\n",
    "            frame = Image.open(f).convert('RGB')\n",
    "            if self.transform is not None:  # BE CAREFUL TRANSFORMATION MIGHT NEED TO CHANGE FOR VIDEO EVAL!!!!!\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        img = torch.stack(frames)  # need to be of the same size!\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            if self.label_map is not None:\n",
    "                label = torch.tensor(self.label_map[(self.labels[index])])\n",
    "            else:\n",
    "                label = torch.tensor(int(self.labels[index]))\n",
    "            return img, label\n",
    "        else:\n",
    "            return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filepaths)\n",
    "\n",
    "\n",
    "class SoftLabelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset used to create dataloaders.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, labels, remove_old_labels=False):\n",
    "        \"\"\"\n",
    "        Create a new SoftLabelDataset.\n",
    "        :param dataset: A PyTorch dataset\n",
    "        :param labels: A list of labels\n",
    "        :param remove_old_labels: A boolean indicating whether to the dataset returns labels that we do not use\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "        self.remove_old_labels = remove_old_labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.dataset[index]\n",
    "        label = self.labels[index]\n",
    "        \n",
    "        if self.remove_old_labels:\n",
    "            data = data[0]\n",
    "            \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "def transform_image(train=True):\n",
    "    \"\"\"\n",
    "    Get the transform to be used on an image.\n",
    "    :return: A transform\n",
    "    \"\"\"\n",
    "    data_mean = [0.485, 0.456, 0.406]\n",
    "    data_std = [0.229, 0.224, 0.225]\n",
    "    # Remember to check it for video and eval\n",
    "    if train:\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=data_mean, std=data_std)\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=data_mean, std=data_std)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_type = 'sample'\n",
    "#train_meta = pd.read_feather('/lwll/development/hmdb/labels_sample')#(f\"{root}labels_{data_type}/meta_train.feather\")\n",
    "#train_ids = train_meta['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = r.json()['Labels']\n",
    "\n",
    "labels_list = []\n",
    "dictionary_clips = {}\n",
    "for clip in labels:\n",
    "    action_frames = [str(clip['id']) + '/' + str(i)+'.jpg' for i in range(clip['start_frame'], clip['end_frame'])]\n",
    "    dictionary_clips[clip[\"id\"]] = action_frames\n",
    "    labels_list.append([clip[\"class\"], clip[\"id\"]])\n",
    "\n",
    "image_labels, image_names  = list(zip(*labels_list))\n",
    "\n",
    "image_paths = [os.path.join(unlabeled_image_path, str(image_name)) for image_name in image_names]\n",
    "\n",
    "\n",
    "\n",
    "if video:\n",
    "    paths_dictionary_clips = {}\n",
    "    for clip, frames in dictionary_clips.items():\n",
    "        paths_dictionary_clips[clip] = [os.path.join(unlabeled_image_path, str(f)) for f in frames]\n",
    "    dictionary_clips = paths_dictionary_clips\n",
    "else:\n",
    "    dictionary_clips = None\n",
    "\n",
    "image_paths = np.asarray(image_paths)\n",
    "image_labels = np.asarray(image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_num = 1\n",
    "\n",
    "if checkpoint_num >= 4:\n",
    "    # 80% for training, 20% for validation\n",
    "    train_percent = 0.8\n",
    "    num_data = len(image_paths)\n",
    "    indices = list(range(num_data))\n",
    "    train_split = int(np.floor(train_percent * num_data))\n",
    "    np.random.shuffle(indices)\n",
    "    train_idx = indices[:train_split]\n",
    "    val_idx = indices[train_split:]\n",
    "else:\n",
    "    train_idx = list(range(len(image_paths)))\n",
    "    val_idx = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {}\n",
    "class_names = r_data.json()['dataset_metadata']['classes']\n",
    "for idx, item in enumerate(class_names):\n",
    "    label_map[item] = idx\n",
    "\n",
    "label_map = label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "if video:\n",
    "    train_dataset = CustomVideoDataset(image_paths[train_idx],\n",
    "                                       labels=image_labels[train_idx],\n",
    "                                       label_map=label_map,\n",
    "                                       transform=transform_image(train=True),\n",
    "                                       clips_dictionary=dictionary_clips)\n",
    "    if len(val_idx) != 0:\n",
    "        val_dataset = CustomVideoDataset(image_paths[val_idx],\n",
    "                                         labels=image_labels[val_idx],\n",
    "                                         label_map=label_map,\n",
    "                                         transform=transform_image(train=False),\n",
    "                                         clips_dictionary=dictionary_clips)\n",
    "    else:\n",
    "        val_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../../lwll/development/hmdb/hmdb_sample/train/154/14941.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-203-a0be1c17c5b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-190-ce1d3051607c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mframes_paths\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# get same size clips - random pick for eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# BE CAREFUL TRANSFORMATION MIGHT NEED TO CHANGE FOR VIDEO EVAL!!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/snap/jupyter/common/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2911\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2912\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2913\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../../lwll/development/hmdb/hmdb_sample/train/154/14941.jpg'"
     ]
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# SlowFast transform\n",
    "####################\n",
    "\n",
    "# We might want different options for transforming the data depending on the model we have\n",
    "\n",
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "transform =  ApplyTransformToKey(\n",
    "    key=\"video\",\n",
    "    transform=Compose(\n",
    "        [\n",
    "            UniformTemporalSubsample(num_frames),\n",
    "            Lambda(lambda x: x/255.0),\n",
    "            NormalizeVideo(mean, std),\n",
    "            ShortSideScale(\n",
    "                size=side_size\n",
    "            ),\n",
    "            CenterCropVideo(crop_size),\n",
    "            PackPathway()\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The duration of the input clip is also specific to the model.\n",
    "clip_duration = (num_frames * sampling_rate)/frames_per_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example video\n",
    "video_path = \"archery.mp4\"\n",
    "\n",
    "# Select the duration of the clip to load by specifying the start and end duration\n",
    "# The start_sec should correspond to where the action occurs in the video\n",
    "start_sec = 0\n",
    "end_sec = start_sec + clip_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an EncodedVideo helper class\n",
    "video = EncodedVideo.from_path(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_video_name': 'archery.mp4',\n",
       " '_decode_audio': True,\n",
       " '_container': <av.InputContainer <_io.BytesIO object at 0x7f57f7973fc0>>,\n",
       " '_video_time_base': Fraction(1, 30000),\n",
       " '_video_start_pts': 0,\n",
       " '_has_audio': (<av.AudioStream #1 aac at 44100Hz, stereo, fltp at 0x7f57f7a086a8>,),\n",
       " '_audio_time_base': Fraction(1, 44100),\n",
       " '_audio_start_pts': 265,\n",
       " '_video': None,\n",
       " '_audio': None,\n",
       " '_selective_decoding': True,\n",
       " '_duration': 10.01}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data = video.get_clip(start_sec=start_sec, end_sec=end_sec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 240, 320])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data['video'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a transform to normalize the video input\n",
    "video_data = transform(video_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 256, 256])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_data['video'][0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prendere una clip e passarla dentro "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prendi con API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (blocks): ModuleList(\n",
       "    (0): MultiPathWayWithFuse(\n",
       "      (multipathway_blocks): ModuleList(\n",
       "        (0): ResNetBasicStem(\n",
       "          (conv): Conv3d(3, 64, kernel_size=(1, 7, 7), stride=(1, 2, 2), padding=(0, 3, 3), bias=False)\n",
       "          (norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (pool): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (1): ResNetBasicStem(\n",
       "          (conv): Conv3d(3, 8, kernel_size=(5, 7, 7), stride=(1, 2, 2), padding=(2, 3, 3), bias=False)\n",
       "          (norm): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (activation): ReLU()\n",
       "          (pool): MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=[0, 1, 1], dilation=1, ceil_mode=False)\n",
       "        )\n",
       "      )\n",
       "      (multipathway_fusion): FuseFastToSlow(\n",
       "        (conv_fast_to_slow): Conv3d(8, 16, kernel_size=(7, 1, 1), stride=(4, 1, 1), padding=(3, 0, 0), bias=False)\n",
       "        (norm): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (1): MultiPathWayWithFuse(\n",
       "      (multipathway_blocks): ModuleList(\n",
       "        (0): ResStage(\n",
       "          (res_blocks): ModuleList(\n",
       "            (0): ResBlock(\n",
       "              (branch1_conv): Conv3d(80, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "              (branch1_norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(80, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResStage(\n",
       "          (res_blocks): ModuleList(\n",
       "            (0): ResBlock(\n",
       "              (branch1_conv): Conv3d(8, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "              (branch1_norm): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(8, 8, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(8, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(32, 8, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(8, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(32, 8, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(8, 8, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(8, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (multipathway_fusion): FuseFastToSlow(\n",
       "        (conv_fast_to_slow): Conv3d(32, 64, kernel_size=(7, 1, 1), stride=(4, 1, 1), padding=(3, 0, 0), bias=False)\n",
       "        (norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (2): MultiPathWayWithFuse(\n",
       "      (multipathway_blocks): ModuleList(\n",
       "        (0): ResStage(\n",
       "          (res_blocks): ModuleList(\n",
       "            (0): ResBlock(\n",
       "              (branch1_conv): Conv3d(320, 512, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
       "              (branch1_norm): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(320, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (3): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(128, 128, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(128, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResStage(\n",
       "          (res_blocks): ModuleList(\n",
       "            (0): ResBlock(\n",
       "              (branch1_conv): Conv3d(32, 64, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
       "              (branch1_norm): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(32, 16, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(16, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(64, 16, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(16, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(64, 16, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(16, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (3): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(64, 16, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(16, 16, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(16, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (multipathway_fusion): FuseFastToSlow(\n",
       "        (conv_fast_to_slow): Conv3d(64, 128, kernel_size=(7, 1, 1), stride=(4, 1, 1), padding=(3, 0, 0), bias=False)\n",
       "        (norm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (3): MultiPathWayWithFuse(\n",
       "      (multipathway_blocks): ModuleList(\n",
       "        (0): ResStage(\n",
       "          (res_blocks): ModuleList(\n",
       "            (0): ResBlock(\n",
       "              (branch1_conv): Conv3d(640, 1024, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
       "              (branch1_norm): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(640, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(1024, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(1024, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (3): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(1024, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (4): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(1024, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (5): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(1024, 256, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(256, 256, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(256, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResStage(\n",
       "          (res_blocks): ModuleList(\n",
       "            (0): ResBlock(\n",
       "              (branch1_conv): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
       "              (branch1_norm): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(64, 32, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(32, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(128, 32, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(32, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(128, 32, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(32, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (3): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(128, 32, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(32, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (4): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(128, 32, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(32, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (5): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(128, 32, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(32, 32, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(32, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (multipathway_fusion): FuseFastToSlow(\n",
       "        (conv_fast_to_slow): Conv3d(128, 256, kernel_size=(7, 1, 1), stride=(4, 1, 1), padding=(3, 0, 0), bias=False)\n",
       "        (norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "    )\n",
       "    (4): MultiPathWayWithFuse(\n",
       "      (multipathway_blocks): ModuleList(\n",
       "        (0): ResStage(\n",
       "          (res_blocks): ModuleList(\n",
       "            (0): ResBlock(\n",
       "              (branch1_conv): Conv3d(1280, 2048, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
       "              (branch1_norm): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(1280, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(2048, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(2048, 512, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(512, 512, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(512, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResStage(\n",
       "          (res_blocks): ModuleList(\n",
       "            (0): ResBlock(\n",
       "              (branch1_conv): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 2, 2), bias=False)\n",
       "              (branch1_norm): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(128, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 2, 2), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(256, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResBlock(\n",
       "              (branch2): BottleneckBlock(\n",
       "                (conv_a): Conv3d(256, 64, kernel_size=(3, 1, 1), stride=(1, 1, 1), padding=(1, 0, 0), bias=False)\n",
       "                (norm_a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_a): ReLU()\n",
       "                (conv_b): Conv3d(64, 64, kernel_size=(1, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1), bias=False)\n",
       "                (norm_b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                (act_b): ReLU()\n",
       "                (conv_c): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
       "                (norm_c): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (multipathway_fusion): Identity()\n",
       "    )\n",
       "    (5): PoolConcatPathway(\n",
       "      (pool): ModuleList(\n",
       "        (0): AvgPool3d(kernel_size=(8, 7, 7), stride=(1, 1, 1), padding=(0, 0, 0))\n",
       "        (1): AvgPool3d(kernel_size=(32, 7, 7), stride=(1, 1, 1), padding=(0, 0, 0))\n",
       "      )\n",
       "    )\n",
       "    (6): ResNetBasicHead(\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (proj): Linear(in_features=2304, out_features=400, bias=True)\n",
       "      (output_pool): AdaptiveAvgPool3d(output_size=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the inputs to the desired device\n",
    "inputs = video_data[\"video\"]\n",
    "inputs = [i.to(device)[None, ...] for i in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the input clip through the model\n",
    "preds = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'talk': 0,\n",
       " 'climb': 1,\n",
       " 'shoot_bow': 2,\n",
       " 'ride_bike': 3,\n",
       " 'pullup': 4,\n",
       " 'ride_horse': 5,\n",
       " 'smile': 6,\n",
       " 'climb_stairs': 7,\n",
       " 'brush_hair': 8,\n",
       " 'pushup': 9,\n",
       " 'walk': 10,\n",
       " 'drink': 11,\n",
       " 'run': 12,\n",
       " 'pick': 13,\n",
       " 'somersault': 14,\n",
       " 'hug': 15,\n",
       " 'fencing': 16,\n",
       " 'sit': 17,\n",
       " 'catch': 18,\n",
       " 'shake_hands': 19,\n",
       " 'hit': 20,\n",
       " 'kiss': 21,\n",
       " 'situp': 22,\n",
       " 'dive': 23,\n",
       " 'sword': 24,\n",
       " 'wave': 25,\n",
       " 'sword_exercise': 26,\n",
       " 'flic_flac': 27,\n",
       " 'throw': 28,\n",
       " 'punch': 29,\n",
       " 'swing_baseball': 30,\n",
       " 'stand': 31,\n",
       " 'shoot_ball': 32,\n",
       " 'laugh': 33,\n",
       " 'kick_ball': 34,\n",
       " 'dribble': 35,\n",
       " 'cartwheel': 36,\n",
       " 'turn': 37,\n",
       " 'handstand': 38,\n",
       " 'shoot_gun': 39,\n",
       " 'draw_sword': 40,\n",
       " 'kick': 41,\n",
       " 'smoke': 42,\n",
       " 'clap': 43,\n",
       " 'chew': 44,\n",
       " 'fall_floor': 45,\n",
       " 'pour': 46,\n",
       " 'eat': 47,\n",
       " 'push': 48,\n",
       " 'jump': 49,\n",
       " 'golf': 50}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks[6].proj.in_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, Lambda\n",
    "from torchvision.transforms._transforms_video import (\n",
    "    CenterCropVideo,\n",
    "    NormalizeVideo,\n",
    ")\n",
    "from pytorchvideo.data.encoded_video import EncodedVideo\n",
    "\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.transforms as video_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    UniformCropVideo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "side_size = 256\n",
    "mean = [0.45, 0.45, 0.45]\n",
    "std = [0.225, 0.225, 0.225]\n",
    "crop_size = 256\n",
    "num_frames = 32\n",
    "sampling_rate = 2\n",
    "frames_per_second = 30\n",
    "alpha = 4\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list\n",
    "\n",
    "def transform_video(self):   \n",
    "    \"\"\"Trasformation valid for SlowFast\"\"\"\n",
    "    side_size = 256\n",
    "    mean = [0.45, 0.45, 0.45]\n",
    "    std = [0.225, 0.225, 0.225]\n",
    "    crop_size = 256\n",
    "    num_frames = 32\n",
    "    sampling_rate = 2\n",
    "    frames_per_second = 30\n",
    "    alpha = 4\n",
    "\n",
    "    return  ApplyTransformToKey(key=\"video\",\n",
    "                                transform=transforms.Compose([\n",
    "                                    UniformTemporalSubsample(num_frames),\n",
    "                                    transforms.Lambda(lambda x: x/255.0),\n",
    "                                    NormalizeVideo(mean, std),\n",
    "                                    ShortSideScale(size=side_size),\n",
    "                                    CenterCropVideo(crop_size),\n",
    "                                    PackPathway()\n",
    "                                ])\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(train=True):\n",
    "    \"\"\"\n",
    "    Get the transform to be used on an image.\n",
    "    :return: A transform\n",
    "    \"\"\"\n",
    "    data_mean = [0.45, 0.45, 0.45]\n",
    "    data_std = [0.225, 0.225, 0.225]\n",
    "    # Remember to check it for video and eval\n",
    "    if train:\n",
    "        return transforms.Compose([\n",
    "            #transforms.RandomResizedCrop((224, 224), scale=(0.8, 1.0)),\n",
    "            #transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=data_mean, std=data_std)\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            #transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize(mean=data_mean, std=data_std)\n",
    "        ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
